{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLU10QfSWXC6"
   },
   "source": [
    "# Assignment 1: Implementing Document Loaders in LangChain\n",
    "\n",
    "## Objective:\n",
    "Write a Python script that uses LangChain’s document loaders to load documents from a directory. Your task is to implement functionality that reads `.txt` and `.pdf` files and outputs their content as LangChain `Document` objects.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n",
    "2. Implement a function `load_documents(directory: str)` that:\n",
    "   - Iterates through all files in the specified directory.\n",
    "   - Loads the content of `.txt` and `.pdf` files.\n",
    "   - Returns a list of `Document` objects, where:\n",
    "     - **Page Content:** Contains the file’s text content.\n",
    "     - **Metadata:** Includes the filename or other relevant file metadata.\n",
    "3. Handle unsupported file types or errors gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## Input:\n",
    "- A directory containing files with extensions `.txt` and `.pdf`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output:\n",
    "- A list of LangChain `Document` objects. Each document should contain:\n",
    "  - The text content of the file.\n",
    "  - Metadata such as the filename.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "A directory with the following files:\n",
    "- `example.txt` containing \"Hello, this is a text file.\"\n",
    "- `example.pdf` containing \"This is a PDF document.\"\n",
    "- `image.jpg` (unsupported file type).\n",
    "\n",
    "### Output:\n",
    "```python\n",
    "[\n",
    "    Document(page_content=\"Hello, this is a text file.\", metadata={\"filename\": \"example.txt\"}),\n",
    "    Document(page_content=\"This is a PDF document.\", metadata={\"filename\": \"example.pdf\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0DnO8NYU38U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Unknown, Content Preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and\n",
      "File: Unknown, Content Preview: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural\n",
      "File: Unknown, Content Preview: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us\n",
      "File: Unknown, Content Preview: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (r\n",
      "File: Unknown, Content Preview: output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "de\n",
      "File: Unknown, Content Preview: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for \n",
      "File: Unknown, Content Preview: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sent\n",
      "File: Unknown, Content Preview: Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "En\n",
      "File: Unknown, Content Preview: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "File: Unknown, Content Preview: Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "File: Unknown, Content Preview: [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua\n",
      "File: Unknown, Content Preview: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "c\n",
      "File: Unknown, Content Preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governm\n",
      "File: Unknown, Content Preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "File: Unknown, Content Preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "File: Unknown, Content Preview: The world must be made safe for democracy. Its peace must be planted upon the tested foundations of \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Skeleton Function: Load .txt and .pdf documents from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            # Placeholder for loading .txt files\n",
    "            if filename.endswith(\".txt\"):\n",
    "             loader= TextLoader(file_path)\n",
    "             documents.extend(loader.load())\n",
    "                 \n",
    "            # Placeholder for loading .pdf files\n",
    "            elif filename.endswith(\".pdf\"):\n",
    "              loader = PyPDFLoader(file_path)\n",
    "              documents.extend(loader.load())\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error for files that could not be loaded\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    # Return the list of documents\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory path\n",
    "    directory_path = r\"C:\\Users\\pavan\\OneDrive\\Desktop\\Rag_Lab\"\n",
    " \n",
    "\n",
    "    # Call the function to load documents\n",
    "    docs = load_documents(directory_path)\n",
    "\n",
    "    # Iterate through the loaded documents and print metadata and content preview\n",
    "    for doc in docs:\n",
    "        print(f\"File: {doc.metadata.get('filename', 'Unknown')}, Content Preview: {doc.page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_YBadNLWsho"
   },
   "source": [
    "# Assignment: Chunking Data and Converting It to Vector Embeddings\n",
    "\n",
    "## Objective:\n",
    "Write a Python script that uses LangChain to:\n",
    "1. Load `.txt` and `.pdf` files as `Document` objects from a directory.\n",
    "2. Chunk the data into smaller pieces for efficient processing.\n",
    "3. Convert the chunks into vector embeddings using a text embedding model.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. **Document Loading**:\n",
    "   - Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n",
    "   - Implement a function `load_documents(directory: str)` to load all files from a directory as LangChain `Document` objects.\n",
    "\n",
    "2. **Chunking**:\n",
    "   - Use LangChain’s `RecursiveCharacterTextSplitter` to split the document text into smaller chunks.\n",
    "   - Implement a function `chunk_documents(documents: List[Document]) -> List[Document]`.\n",
    "\n",
    "3. **Embedding Generation**:\n",
    "   - Use a pre-trained embedding model (e.g., `OpenAIEmbeddings` or any other LangChain-compatible embedding model).\n",
    "   - Implement a function `generate_embeddings(chunks: List[Document]) -> List[List[float]]` that converts each chunk into a vector embedding.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Handle unsupported file types and errors gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## Input:\n",
    "- A directory containing `.txt` and `.pdf` files.\n",
    "\n",
    "---\n",
    "\n",
    "## Output:\n",
    "- A list of vector embeddings for the chunks of the loaded documents.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "A directory with the following files:\n",
    "- `example.txt` containing \"This is an example text file.\"\n",
    "- `example.pdf` containing \"This is an example PDF document.\"\n",
    "\n",
    "### Output:\n",
    "A list of embeddings (e.g., 768-dimensional vectors) for the chunks generated from the documents.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXB9pEuNXS_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1: [0.0075033302, -0.012546867, -0.00985978, -0.0059293485, -0.005868988, -0.01740459, 0.010194887, 0.0021364293, -0.014074748, -0.013853891]...\n",
      "Embedding 2: [0.004858137, -0.010867439, -0.01785298, -0.0008316965, -0.020331746, -0.029635351, -0.0011648479, -0.01588062, 0.008147696, -0.01155419]...\n",
      "Embedding 3: [0.012772601, -0.006307596, -0.006896984, 0.005971319, -0.02572523, -0.00057803775, 0.014220787, 0.00056576275, 0.0016497367, 0.0003296631]...\n",
      "Embedding 4: [-0.010720819, -0.028336309, 0.024151672, -0.0037161945, -0.0050437474, 0.0047234343, 0.005227068, -0.0150557, 0.026371628, -0.0091641955]...\n",
      "Embedding 5: [-0.0079080295, -0.029269118, 0.01162099, -0.00971782, -0.02498191, 0.00015435182, 0.022947038, -0.02820296, -0.002049774, -0.01838087]...\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from typing import List\n",
    "\n",
    "def chunk_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of LangChain Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of chunked Document objects.\n",
    "    \"\"\"\n",
    "    # Create an instance of the text splitter with specified chunk size and overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate over each document and split it into chunks\n",
    "    for doc in documents:\n",
    "        # Split the document and add chunks to the list\n",
    "        chunks.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(chunks: List[Document]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generates vector embeddings for the given chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[Document]): List of chunked Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: A list of vector embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI embeddings model\n",
    "    embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    return [embeddings.embed_query(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "# Defining load document \n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Function to load .txt and .pdf documents from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            # Load .txt files\n",
    "            if filename.endswith(\".txt\"):\n",
    "                loader = TextLoader(file_path)\n",
    "                documents.extend(loader.load())  # Use extend to append multiple documents\n",
    "            \n",
    "            # Load .pdf files\n",
    "            elif filename.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents.extend(loader.load())  # Use extend to append multiple documents\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error for files that could not be loaded\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    # Return the list of documents\n",
    "    return documents\n",
    "        \n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Correct file path\n",
    "    directory_path = r\"C:\\Users\\pavan\\OneDrive\\Desktop\\Rag_Lab\"  # Make sure the path is valid\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(directory_path)\n",
    "\n",
    "    if documents:\n",
    "        # Chunk the documents into smaller chunks\n",
    "        chunks = chunk_documents(documents)\n",
    "\n",
    "        # Generate embeddings for the chunks\n",
    "        embeddings = generate_embeddings(chunks)\n",
    "\n",
    "        # Display first 5 embeddings for demonstration\n",
    "        for i, embedding in enumerate(embeddings[:5]):  # Display first 5 embeddings for brevity\n",
    "            print(f\"Embedding {i + 1}: {embedding[:10]}...\")  # Print first 10 dimensions for brevity\n",
    "    else:\n",
    "        print(\"No documents loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24KvzOpuYdHC"
   },
   "source": [
    "# Assignment 3: Creating and Querying a Vector Database with Chroma\n",
    "\n",
    "## Objective:\n",
    "Write a Python script to:\n",
    "1. Create a vector database using the **Chroma** library.\n",
    "2. Store vector embeddings of document chunks in the database.\n",
    "3. Query the database using similarity search and retrieve the top `k` results.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. **Vector Database Creation**:\n",
    "   - Use Chroma to create a persistent vector database.\n",
    "   - Add document embeddings (e.g., from OpenAI or any other embedding model) along with metadata to the database.\n",
    "\n",
    "2. **Similarity Search**:\n",
    "   - Implement a function to query the database with a user-provided text and retrieve the top `k` most similar results.\n",
    "\n",
    "3. **Input Data**:\n",
    "   - Use a list of text chunks or embeddings for this task. You may generate these from documents (e.g., `.txt` or `.pdf` files).\n",
    "\n",
    "4. **Outputs**:\n",
    "   - Return the metadata and content of the top `k` most similar results from the database.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "1. A collection of text chunks from documents such as:\n",
    "   - `\"LangChain is a framework for developing applications powered by LLMs.\"`\n",
    "   - `\"Chroma is a vector database used for storing embeddings and performing similarity search.\"`\n",
    "   - `\"Document loaders are part of LangChain and help load data from multiple formats.\"`\n",
    "\n",
    "2. Query text: `\"What is Chroma?\"`\n",
    "3. `k=2`\n",
    "\n",
    "### Output:\n",
    "Top `k` results based on similarity:\n",
    "1. Content: `\"Chroma is a vector database used for storing embeddings and performing similarity search.\"`\n",
    "   Metadata: `{...}`\n",
    "2. Content: `\"LangChain is a framework for developing applications powered by LLMs.\"`\n",
    "   Metadata: `{...}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6ujM-v5hYeAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\n",
      "\n",
      "Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\n",
      "\n",
      "…\n",
      "\n",
      "It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\n",
      "\n",
      "We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\n",
      "\n",
      "It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\n",
      "\n",
      "To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "\n",
    "def load_documents(directory_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads text files from a directory and creates LangChain Document objects.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing text files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                documents.append(Document(page_content=content, metadata={\"filename\": filename}))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def initialize_faiss_with_ollama(documents: List[Document], db_path: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Initializes a FAISS vector database with Ollama embeddings and stores documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of LangChain Document objects.\n",
    "        db_path (str): Path to store the FAISS database.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: FAISS vector store object.\n",
    "    \"\"\"\n",
    "    # Initialize Ollama embeddings\n",
    "    ollama_embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    # Create FAISS vector store and store documents\n",
    "    faiss_db = FAISS.from_documents(documents, ollama_embeddings)\n",
    "\n",
    "    # Save the FAISS index to the specified path\n",
    "    faiss_db.save_local(db_path)\n",
    "\n",
    "    return faiss_db\n",
    "\n",
    "def query_faiss(query: str, faiss_db: FAISS, k: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Queries the FAISS database for the top-k similar documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): Query text.\n",
    "        faiss_db (FAISS): FAISS vector store object.\n",
    "        k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of top-k document contents from the database.\n",
    "    \"\"\"\n",
    "    # Perform similarity search in the database\n",
    "    retrieved_results = faiss_db.similarity_search(query, k=k)\n",
    "\n",
    "    # Collect the content of the retrieved documents\n",
    "    results = [result.page_content for result in retrieved_results]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = r\"C:\\Users\\pavan\\OneDrive\\Documents\\ragpdf\"\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(directory_path)\n",
    "\n",
    "    # Initialize FAISS with Ollama embeddings\n",
    "    db_path = r\"C:\\Users\\pavan\\OneDrive\\Desktop\\Rag_Lab/faiss_index\"\n",
    "    faiss_db = initialize_faiss_with_ollama(documents, db_path)\n",
    "\n",
    "    # Query FAISS database\n",
    "    query_text = \"What is FAISS?\"\n",
    "    top_k = 2\n",
    "    results = query_faiss(query_text, faiss_db, top_k)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFCsdV0cdDj35jvdkzi2u9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
